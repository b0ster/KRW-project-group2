{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Extracting Shapley DNN Semantics - EXPERIMENT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src = 'https://media.nationalgeographic.org/assets/photos/000/273/27302_c0-41-990-701_r1050x700.jpg?d4ccf3044d9da0d0118103be3a76bd1319370847' >"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing all necessary libraries\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import enum\n",
    "import os\n",
    "import platform\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.io as pio\n",
    "import pickle\n",
    "from scipy.stats import mannwhitneyu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting random seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the possible nodes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Mode(enum.Enum):\n",
    "    TRAIN = \"TRAIN\"\n",
    "    TEST = \"TEST\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the current run-mode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PC_NAME = platform.node().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "MODEL_DIR = f\"../../output/titanic/models/experiment/{PC_NAME}\"\n",
    "PLOT_DIR = f\"{MODEL_DIR}/plots\"\n",
    "MODE = Mode.TRAIN\n",
    "if MODE == Mode.TEST and not os.path.exists(MODEL_DIR):\n",
    "    print(f\"Error: model not created (path \\\"{MODEL_DIR}\\\" does not exist), mode will be Mode.TRAIN\")\n",
    "    MODE = Mode.TRAIN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_and_save_current_matplotlib_plt(plotname: str):\n",
    "    if not os.path.exists(PLOT_DIR):\n",
    "        os.makedirs(PLOT_DIR)\n",
    "    file_n = f\"{PLOT_DIR}/{plotname}\"\n",
    "    plt.savefig(file_n)\n",
    "    plt.show()\n",
    "\n",
    "def show_and_save_current_plotly_plt(plotname: str, fig, show=True):\n",
    "    if not os.path.exists(PLOT_DIR):\n",
    "        os.makedirs(PLOT_DIR)\n",
    "    file_n = f\"{PLOT_DIR}/{plotname}\"\n",
    "    if show:\n",
    "        fig.show()\n",
    "    pio.write_image(fig, file_n)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the network"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.069583,
     "end_time": "2020-12-21T07:57:07.798284",
     "exception": false,
     "start_time": "2020-12-21T07:57:07.728701",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dropping PassengerId"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titanic_data = pd.read_csv('../../input/titanic/experiment/titanic-experiment-preprocessed.csv')\n",
    "columns_to_drop = ['PassengerId']\n",
    "titanic_data.drop(columns_to_drop, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting train and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = titanic_data.drop(\"Survived\", axis=1)\n",
    "Y_train = titanic_data[\"Survived\"]\n",
    "\n",
    "# Split data into 85% training and 15% testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_train, Y_train, test_size=0.15, shuffle=True, random_state=999)\n",
    "\n",
    "# Convert Y_train and Y_test to categorical\n",
    "Y_train = to_categorical(np.array(Y_train), num_classes=2)\n",
    "Y_test = to_categorical(np.array(Y_test), num_classes=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_inputs = len(X_train.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining model\n",
    "Here, I have used different number of neurons for each layer and different value for dropout. You can play with these hyperparameter for better outut."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=16, input_shape=(n_inputs,), activation='relu'))\n",
    "    model.add(Dense(units=32, activation='relu', kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(Dense(units=2, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compiling and fitting model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "if MODE == Mode.TRAIN:\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "    model.fit(X_train, Y_train, batch_size=16, verbose=2, epochs=1000)\n",
    "    model.save(MODEL_DIR)\n",
    "else:\n",
    "    model.load_weights(MODEL_DIR)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction for test data"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.089203,
     "end_time": "2020-12-21T07:57:19.415297",
     "exception": false,
     "start_time": "2020-12-21T07:57:19.326094",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "Y_pred_rand = [0 if x[0] > x[1] else 1 for x in model.predict(X_test)]\n",
    "Y_test_flat = [0 if x[0] > x[1] else 1 for x in Y_test]\n",
    "print(\"*\" * 10, ' TEST RESULTS ', \"*\" * 10)\n",
    "print('Precision : ', np.round(metrics.precision_score(Y_test_flat, Y_pred_rand) * 100, 2))\n",
    "print('Accuracy : ', np.round(metrics.accuracy_score(Y_test_flat, Y_pred_rand) * 100, 2))\n",
    "print('Recall : ', np.round(metrics.recall_score(Y_test_flat, Y_pred_rand) * 100, 2))\n",
    "print('F1 score : ', np.round(metrics.f1_score(Y_test_flat, Y_pred_rand) * 100, 2))\n",
    "print('AUC : ', np.round(metrics.roc_auc_score(Y_test_flat, Y_pred_rand) * 100, 2))\n",
    "print(\"*\" * 10, ' TEST RESULTS ', \"*\" * 10)"
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-21T07:57:20.572441Z",
     "iopub.status.busy": "2020-12-21T07:57:20.571367Z",
     "iopub.status.idle": "2020-12-21T07:57:20.651419Z",
     "shell.execute_reply": "2020-12-21T07:57:20.65087Z"
    },
    "papermill": {
     "duration": 0.179237,
     "end_time": "2020-12-21T07:57:20.651541",
     "exception": false,
     "start_time": "2020-12-21T07:57:20.472304",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# plotting the confusion matrix in heatmap\n",
    "matrix = metrics.confusion_matrix(Y_test_flat, Y_pred_rand)\n",
    "sns.heatmap(matrix, annot=True, fmt='g')\n",
    "show_and_save_current_matplotlib_plt(\"model_confusion_matrix.png\")"
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-21T07:57:20.841145Z",
     "iopub.status.busy": "2020-12-21T07:57:20.840338Z",
     "iopub.status.idle": "2020-12-21T07:57:21.030101Z",
     "shell.execute_reply": "2020-12-21T07:57:21.030647Z"
    },
    "papermill": {
     "duration": 0.286179,
     "end_time": "2020-12-21T07:57:21.030791",
     "exception": false,
     "start_time": "2020-12-21T07:57:20.744612",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Co-Activation Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = X_test\n",
    "\n",
    "# Compute intermediate layer activations\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "outputs = [model.get_layer(name).output for name in layer_names]\n",
    "model_reduced = tf.keras.models.Model(inputs=model.inputs, outputs=outputs)\n",
    "activations = model_reduced.predict(x)\n",
    "\n",
    "# Compute model predictions on test data\n",
    "predictions = [x for x in model.predict(x)]\n",
    "\n",
    "# Concatenate predictions to activations\n",
    "# Compute pairwise correlations between activations\n",
    "activations_flat = np.concatenate([a.reshape(a.shape[0], -1) for a in activations], axis=1)\n",
    "activations_flat = np.array([np.append(a, p) for a, p in zip(activations_flat, predictions)])\n",
    "correlations = np.corrcoef(activations_flat, rowvar=False)\n",
    "\n",
    "# Replace NaN values with 0 and remove corresponding rows and columns\n",
    "correlations = np.nan_to_num(correlations)\n",
    "nan_idx = np.where(np.isnan(correlations))[0]\n",
    "if len(nan_idx) > 0:\n",
    "    correlations = np.delete(correlations, nan_idx, axis=0)\n",
    "    correlations = np.delete(correlations, nan_idx, axis=1)\n",
    "    layer_names = [name for i, name in enumerate(layer_names) if i not in nan_idx]\n",
    "\n",
    "# Plot co-activation matrix\n",
    "plt.imshow(correlations, cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('Co-activation matrix')\n",
    "plt.xticks(np.arange(len(layer_names)), layer_names, rotation=90)\n",
    "plt.yticks(np.arange(len(layer_names)), layer_names)\n",
    "show_and_save_current_matplotlib_plt(\"co_activation_matrix.png\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.colors as mcolors\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "threshold = 0.4\n",
    "\n",
    "num_nodes = correlations.shape[0]\n",
    "survived_output = num_nodes-1\n",
    "died_output = num_nodes-2\n",
    "\n",
    "# Define the node names as a list\n",
    "# Create a dictionary that maps node indices to node names\n",
    "node_labels = {i: (X_train.columns[i] if i < len(X_train.columns) else '') for i in range(num_nodes-2)}\n",
    "node_labels.update({survived_output: \"Survived\"})\n",
    "node_labels.update({died_output: \"Died\"})\n",
    "\n",
    "min_v = 0.1\n",
    "# Create the graph\n",
    "G = nx.Graph()\n",
    "for i in range(correlations.shape[0]):\n",
    "    for j in range(i + 1, correlations.shape[1]):\n",
    "        corr = min_v if (abs(correlations[i, j])) < min_v else correlations[i, j]\n",
    "        G.add_edge(i, j, weight=corr)\n",
    "\n",
    "# resolution 1.2 to slightly bias more cluster division, threshold 0.5 as research supports this\n",
    "clusters = louvain_communities(G, weight='weight', resolution=1.2, threshold=0.5, seed=123)\n",
    "\n",
    "# Draw the graph\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "pos = nx.spring_layout(G, weight='weight', k=1.3)\n",
    "# nx.draw_networkx_nodes(G, pos, node_color='r', node_size=)\n",
    "nx.draw_networkx_labels(G, pos, font_size=7, labels=node_labels, font_weight='bold')\n",
    "\n",
    "edges, weights = zip(*nx.get_edge_attributes(G, 'weight').items())\n",
    "edges = list(edges)\n",
    "weights = list(weights)\n",
    "\n",
    "# this part will do the edge colorization\n",
    "min_w, max_w = -1, 1\n",
    "red = mcolors.hex2color('#FF0000')  # red color in RGB format\n",
    "green = mcolors.hex2color('#00FF00')  # green color in RGB format\n",
    "edge_colors = []\n",
    "for w in weights:\n",
    "    if abs(w) <= threshold:\n",
    "        edge_colors.append('#00000000')\n",
    "    else:\n",
    "        # Map the weight value to a value between 0 and 1\n",
    "        normalized_w = (w - min_w) / (max_w - min_w)\n",
    "\n",
    "        # Use a linear interpolation to determine the color between red and green\n",
    "        color = tuple((1 - normalized_w) * c1 + normalized_w * c2 for c1, c2 in zip(red, green))\n",
    "\n",
    "        # Convert the color to hexadecimal format and append it to the list\n",
    "        edge_colors.append(mcolors.rgb2hex(color))\n",
    "\n",
    "# draw the network edges\n",
    "edges, weights = zip(*[(edges[i], round(weights[i], 1)) for i in range(len(edges)) if abs(weights[i]) >= threshold])\n",
    "nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color=edge_colors, width=weights, edge_cmap=plt.cm.coolwarm)\n",
    "\n",
    "# draw the louvain communities\n",
    "colors = {}\n",
    "for i, community in enumerate(clusters):\n",
    "    color = plt.cm.Set1(i / len(clusters))\n",
    "    colors.update({node: color for node in community})\n",
    "\n",
    "for c in clusters:\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=list(c), node_color=colors[list(c)[0]], node_size=1000)\n",
    "\n",
    "plt.title(f'Co-Activation graph (edge correlation threshold=$abs(correlation)>={threshold}$)')\n",
    "show_and_save_current_matplotlib_plt(\"co_activation_graph.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SHAP (SHapley Additive exPlanations)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shap\n",
    "import logging\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "recalc = False\n",
    "logging.getLogger('shap').setLevel(logging.WARNING)\n",
    "\n",
    "shap_values_full_model_fname = f\"{MODEL_DIR}/shap_values_full_model.pkl\"\n",
    "shap_values_full_per_cluster_fname = f\"{MODEL_DIR}/shap_values_per_cluster.pkl\"\n",
    "shap_values_full_per_feature_fname = f\"{MODEL_DIR}/shap_values_per_feature.pkl\"\n",
    "shap_values_full_per_feature_dict_fname = f\"{MODEL_DIR}/shap_values_per_feature_dict.pkl\"\n",
    "p_values_per_cluster_per_feature_fname = f\"{MODEL_DIR}/p_values_per_cluster_per_feature.pkl\"\n",
    "significant_divergent_shap_distributions_fname = f\"{MODEL_DIR}/significant_divergent_shap_distributions.pkl\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample_size_full_model():\n",
    "    return 500\n",
    "\n",
    "\n",
    "def relative_node_sample_size(cluster_size):\n",
    "    return sample_size_full_model() / cluster_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full SHAP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def do_or_load_and_write(func, file):\n",
    "    if not recalc and os.path.exists(file):\n",
    "        print(f'loading from file {file}')\n",
    "        with open(file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        print(f'not loading from file {file}, running func and returning...')\n",
    "        r = func()\n",
    "        with open(file, 'wb') as f:\n",
    "            pickle.dump(r, f)\n",
    "        return r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture test\n",
    "\n",
    "# This is the full SHAP explainer (begin layer to end layer)\n",
    "def _2_class_to_binary_f(X, model):\n",
    "    return np.array([0 if x[0] > x[1] else 1 for x in model.predict(X)])\n",
    "\n",
    "\n",
    "def calc_full_shap():\n",
    "    full_explainer = shap.Explainer(lambda x: _2_class_to_binary_f(x, model), X_test)\n",
    "    sample_size = sample_size_full_model()\n",
    "    print(f\"Using full-model sample size {sample_size_full_model()}\")\n",
    "    return full_explainer(shap.utils.sample(X_test, sample_size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture test\n",
    "full_shap_values = do_or_load_and_write(calc_full_shap, shap_values_full_model_fname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_aggregated_model_shap(clazz, agg_func=lambda x: np.mean(np.abs(x))) -> dict:\n",
    "    assert clazz in {0, 1}\n",
    "    return {f: agg_func((full_shap_values.values[:, idx])) * (1 if clazz == 1 else -1) for idx, f in\n",
    "            enumerate(X_train.columns)}\n",
    "\n",
    "\n",
    "survived_mean_shap_vals = get_aggregated_model_shap(clazz=1)\n",
    "died_mean_shap_vals = get_aggregated_model_shap(clazz=0)\n",
    "full_model_shap_values = do_or_load_and_write(lambda: get_aggregated_model_shap(1, lambda x: x), shap_values_full_per_feature_dict_fname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_model_shap_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(full_shap_values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.bar(died_mean_shap_vals.keys(), died_mean_shap_vals.values())\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('SHAP value')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('SHAP values per feature on class 0 (\"died\")')\n",
    "show_and_save_current_matplotlib_plt(\"model_shap_values_bar_chart_died.png\")\n",
    "\n",
    "plt.bar(survived_mean_shap_vals.keys(), survived_mean_shap_vals.values())\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('SHAP value')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('SHAP values per feature on class 1 (\"survived\")')\n",
    "show_and_save_current_matplotlib_plt(\"model_shap_values_bar_chart_survived.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clustered SHAP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "named_clusters = {f\"cluster_{i}\": c for i, c in enumerate(clusters)}\n",
    "print(named_clusters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Steps new SHAP-research idea\n",
    "1. Per node, per instance, activation SHAP values (per instance)\n",
    "2. Per cluster, per feature, SHAP distribution\n",
    "3. General SHAP values per feature on entire model\n",
    "4. Check where general SHAP values per feature lay within the distribution per feature per cluster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nodes = set()\n",
    "for c in clusters:\n",
    "    for node in c:\n",
    "        nodes.add(node)\n",
    "print(f\"Number of nodes: {len(nodes)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_function_per_node = {}\n",
    "node_num = 0\n",
    "for layer_id, layer in enumerate(model.layers):\n",
    "    m = tf.keras.Model(inputs=model.input, outputs=layer.output)\n",
    "    for i in range(layer.output.shape[1]):\n",
    "        if node_num in nodes:\n",
    "            model_function_per_node[node_num] = m\n",
    "        node_num += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def node_activation_f(X, node: int):\n",
    "    if node in model_function_per_node:\n",
    "        m = model_function_per_node[node]\n",
    "        local_node_num = node % m.output.shape[1]\n",
    "        return np.array([x[local_node_num] for x in m.predict(X)])\n",
    "    raise Exception(f\"{n} not in {model_function_per_node.keys()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture test\n",
    "nodes = [n for n in model_function_per_node.keys()]\n",
    "\n",
    "clusters_size_per_node = {}\n",
    "for c in clusters:\n",
    "    for n in c:\n",
    "        clusters_size_per_node[n] = len(c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_shap_per_feature():\n",
    "    shap_values_per_feature = {x: {n: [] for n in nodes} for x in X_train.columns}\n",
    "\n",
    "    current_node_number = nodes[0]\n",
    "    ex = shap.Explainer(lambda x: node_activation_f(x, current_node_number), X_train)\n",
    "    for n in nodes:\n",
    "        sample_size = int(np.ceil(relative_node_sample_size(clusters_size_per_node[n])))\n",
    "        sample = shap.utils.sample(X_test, sample_size)\n",
    "        current_node_number = n\n",
    "        vals = ex(sample).values\n",
    "        for svarr in vals:\n",
    "            for idx, sv in enumerate(svarr, 0):\n",
    "                shap_values_per_feature[X_train.columns[idx]][n].append(sv)\n",
    "    return shap_values_per_feature"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture test\n",
    "shap_values_per_feature = do_or_load_and_write(calc_shap_per_feature, shap_values_full_per_feature_fname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_shap_per_cluster():\n",
    "    shap_values_per_cluster = {c: {f: [] for f in X_train.columns} for c in named_clusters.keys()}\n",
    "    for feature, n_sv in shap_values_per_feature.items():\n",
    "        for node, sv in n_sv.items():\n",
    "            for c_name, nodes in named_clusters.items():\n",
    "                if node in nodes:\n",
    "                    for _sv in sv:\n",
    "                        shap_values_per_cluster[c_name][feature].append(_sv)\n",
    "    return shap_values_per_cluster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap_values_per_cluster = do_or_load_and_write(calc_shap_per_cluster, shap_values_full_per_cluster_fname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "#real deal\n",
    "n_bins = 50\n",
    "y_range = [0,0.15]\n",
    "x_range = [-2,2]\n",
    "normalise = 'probability'\n",
    "\n",
    "for cluster_name, feats_with_shaps  in shap_values_per_cluster.items():\n",
    "\n",
    "    for feat_name, shap_values in feats_with_shaps.items():\n",
    "\n",
    "        #main object\n",
    "        fig = go.Figure()\n",
    "\n",
    "        #first data (cluster specific)\n",
    "        fig.add_trace(go.Histogram(\n",
    "            x=shap_values, #shaps of feature of cluster\n",
    "            histnorm=normalise,\n",
    "            name=f'{feat_name} of {cluster_name}',\n",
    "            nbinsx=n_bins,\n",
    "            # marker_color='blue',\n",
    "        ))\n",
    "\n",
    "        #data for second plot: shaps of feature of the whole model\n",
    "        fig.add_trace(go.Histogram(\n",
    "            x=full_model_shap_values[feat_name],\n",
    "            histnorm=normalise,\n",
    "            name=f'{feat_name} of the actual model',\n",
    "            nbinsx=n_bins,\n",
    "            # marker_color='#EB89B5',\n",
    "        ))\n",
    "\n",
    "\n",
    "        #to tweak appearances\n",
    "        fig.update_layout(barmode='overlay',\n",
    "            title_text=f'Distributions of {feat_name} in {cluster_name} against whole model', # title of plot\n",
    "            xaxis_title_text='Shapley value', # xaxis label\n",
    "            yaxis_title_text='percentage', # yaxis label\n",
    "\n",
    "            yaxis=dict(\n",
    "                range=y_range\n",
    "            ),\n",
    "            xaxis=dict(\n",
    "                range=x_range\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Reduce opacity to see both histograms\n",
    "        fig.update_traces(opacity=0.7)\n",
    "        show_and_save_current_plotly_plt(f'histg_{(feat_name.lower())}_{cluster_name}.png', fig, show=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "#real deal\n",
    "n_bins = 50\n",
    "y_range = [0,0.15]\n",
    "x_range = [-2,2]\n",
    "normalise = 'probability'\n",
    "\n",
    "shap_values_per_feature_per_cluster  = {}\n",
    "\n",
    "for clus,sfc_dict in shap_values_per_cluster.items():\n",
    "    for feat, sv in sfc_dict.items():\n",
    "        if feat in shap_values_per_feature_per_cluster:\n",
    "            shap_values_per_feature_per_cluster[feat][clus] = sv\n",
    "        else:\n",
    "            shap_values_per_feature_per_cluster[feat] = {}\n",
    "            shap_values_per_feature_per_cluster[feat][clus] = sv\n",
    "\n",
    "for feat, sv_cluster  in shap_values_per_feature_per_cluster.items():\n",
    "\n",
    "        #main object\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for clus, sv in sv_cluster.items():\n",
    "            #first data (cluster specific)\n",
    "            fig.add_trace(go.Histogram(\n",
    "                x=sv, #shaps of feature of cluster\n",
    "                histnorm=normalise,\n",
    "                name=f'{feat} of {clus}',\n",
    "                nbinsx=n_bins,\n",
    "                # marker_color='blue',\n",
    "            ))\n",
    "\n",
    "        #data for second plot: shaps of feature of the whole model\n",
    "        fig.add_trace(go.Histogram(\n",
    "            x=full_model_shap_values[feat],\n",
    "            histnorm=normalise,\n",
    "            name=f'{feat} of the actual model',\n",
    "            nbinsx=n_bins,\n",
    "            # marker_color='#EB89B5',\n",
    "        ))\n",
    "\n",
    "\n",
    "        #to tweak appearances\n",
    "        fig.update_layout(barmode='overlay',\n",
    "            title_text=f'Distributions of {feat} in all clusters against whole model', # title of plot\n",
    "            xaxis_title_text='Shapley value', # xaxis label\n",
    "            yaxis_title_text='percentage', # yaxis label\n",
    "\n",
    "            yaxis=dict(\n",
    "                range=y_range\n",
    "            ),\n",
    "            xaxis=dict(\n",
    "                range=x_range\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Reduce opacity to see both histograms\n",
    "        fig.update_traces(opacity=0.7)\n",
    "        show_and_save_current_plotly_plt(f'histg_{(feat.lower())}_clusters_combined.png', fig, show=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding significant different distributions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#adjust for correct model\n",
    "shaps_full_model = full_model_shap_values\n",
    "shaps_per_cluster = shap_values_per_cluster\n",
    "shaps_per_feature = shap_values_per_feature_per_cluster\n",
    "\n",
    "def find_pvalues_per_feature(shaps_full_model, dict_of_features_with_shaps):\n",
    "    '''\n",
    "    Calc mann-withney for every feature against full_model\n",
    "\n",
    "    Returns: dict with p-values per feature\n",
    "    '''\n",
    "    dict_of_pvalues_per_feature = {}\n",
    "\n",
    "    #loop over available features\n",
    "    for feature_name, shapley_values in dict_of_features_with_shaps.items():\n",
    "\n",
    "        #calc M-W two sided (same or different)\n",
    "        _ , p_value = mannwhitneyu(shaps_full_model[feature_name], shapley_values, alternative='two-sided')\n",
    "\n",
    "        dict_of_pvalues_per_feature[feature_name] = round(p_value,4)\n",
    "\n",
    "    return dict_of_pvalues_per_feature\n",
    "\n",
    "\n",
    "def find_p_values_per_cluster_per_feature():\n",
    "    p_values_per_cluster_per_feature = {}\n",
    "\n",
    "    #loop over every cluster\n",
    "    for cluster_name in shaps_per_cluster.keys():\n",
    "        result = find_pvalues_per_feature(shaps_full_model=shaps_full_model,\n",
    "                                          dict_of_features_with_shaps=shaps_per_cluster[cluster_name])\n",
    "        p_values_per_cluster_per_feature[cluster_name] = result\n",
    "\n",
    "    return p_values_per_cluster_per_feature\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_values_per_cluster_per_feature = do_or_load_and_write(find_p_values_per_cluster_per_feature, p_values_per_cluster_per_feature_fname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(p_values_per_cluster_per_feature)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filter_significant_differences(p_values_per_cluster_per_feature, signicance_level):\n",
    "    significance_reporter_per_cluster = {cluster_name:[] for cluster_name in p_values_per_cluster_per_feature.keys()}\n",
    "\n",
    "    for cluster_name, dict_of_pvalues_per_feature in p_values_per_cluster_per_feature.items():\n",
    "        for feature_name, p_value in dict_of_pvalues_per_feature.items():\n",
    "            if p_value <= signicance_level:\n",
    "                significance_reporter_per_cluster[cluster_name].append(feature_name)\n",
    "    return significance_reporter_per_cluster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filter_significant_differences_result = do_or_load_and_write(lambda:filter_significant_differences(p_values_per_cluster_per_feature, 0.05), significant_divergent_shap_distributions_fname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filter_significant_differences_result"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
