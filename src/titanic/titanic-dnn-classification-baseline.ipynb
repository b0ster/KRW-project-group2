{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Extracting Shapley DNN Semantics - BASELINE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src = 'https://media.nationalgeographic.org/assets/photos/000/273/27302_c0-41-990-701_r1050x700.jpg?d4ccf3044d9da0d0118103be3a76bd1319370847' >"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing all necessary libraries\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import enum\n",
    "import os\n",
    "import platform\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting random seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the possible nodes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Mode(enum.Enum):\n",
    "    TRAIN = \"TRAIN\"\n",
    "    TEST = \"TEST\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the current run-mode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "PC_NAME = platform.node().replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "MODEL_DIR = f\"../../output/titanic/models/baseline/{PC_NAME}\"\n",
    "PLOT_DIR = f\"{MODEL_DIR}/plots\"\n",
    "MODE = Mode.TEST\n",
    "if MODE == Mode.TEST and not os.path.exists(MODEL_DIR):\n",
    "    print(f\"Error: model not created (path \\\"{MODEL_DIR}\\\" does not exist), mode will be Mode.TRAIN\")\n",
    "    MODE = Mode.TRAIN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def show_and_save_current_plt(plotname: str):\n",
    "    if not os.path.exists(PLOT_DIR):\n",
    "        os.makedirs(PLOT_DIR)\n",
    "    file_n = f\"{PLOT_DIR}/{plotname}\"\n",
    "    plt.savefig(file_n)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the network"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.069583,
     "end_time": "2020-12-21T07:57:07.798284",
     "exception": false,
     "start_time": "2020-12-21T07:57:07.728701",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dropping PassengerId"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "titanic_data = pd.read_csv('../../input/titanic/baseline/titanic-baseline-preprocessed.csv')\n",
    "columns_to_drop = ['PassengerId']\n",
    "titanic_data.drop(columns_to_drop, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting train and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_train = titanic_data.drop(\"Survived\", axis=1)\n",
    "Y_train = titanic_data[\"Survived\"]\n",
    "\n",
    "# Split data into 85% training and 15% testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_train, Y_train, test_size=0.15, shuffle=True)\n",
    "\n",
    "# Convert Y_train and Y_test to categorical\n",
    "Y_train = to_categorical(np.array(Y_train), num_classes=2)\n",
    "Y_test = to_categorical(np.array(Y_test), num_classes=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "n_inputs = len(X_train.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining model\n",
    "Here, I have used different number of neurons for each layer and different value for dropout. You can play with these hyperparameter for better outut."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=16, input_shape=(n_inputs,), activation='relu'))\n",
    "    model.add(Dense(units=32, activation='relu', kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(Dense(units=2, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compiling and fitting model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "if MODE == Mode.TRAIN:\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "    model.fit(X_train, Y_train, batch_size=16, verbose=2, epochs=100)\n",
    "    model.save(MODEL_DIR)\n",
    "else:\n",
    "    model.load_weights(MODEL_DIR)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction for test data"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.089203,
     "end_time": "2020-12-21T07:57:19.415297",
     "exception": false,
     "start_time": "2020-12-21T07:57:19.326094",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "Y_pred_rand = [0 if x[0] > x[1] else 1 for x in model.predict(X_test)]\n",
    "Y_test_flat = [0 if x[0] > x[1] else 1 for x in Y_test]\n",
    "print(\"*\" * 10, ' TEST RESULTS ', \"*\" * 10)\n",
    "print('Precision : ', np.round(metrics.precision_score(Y_test_flat, Y_pred_rand) * 100, 2))\n",
    "print('Accuracy : ', np.round(metrics.accuracy_score(Y_test_flat, Y_pred_rand) * 100, 2))\n",
    "print('Recall : ', np.round(metrics.recall_score(Y_test_flat, Y_pred_rand) * 100, 2))\n",
    "print('F1 score : ', np.round(metrics.f1_score(Y_test_flat, Y_pred_rand) * 100, 2))\n",
    "print('AUC : ', np.round(metrics.roc_auc_score(Y_test_flat, Y_pred_rand) * 100, 2))\n",
    "print(\"*\" * 10, ' TEST RESULTS ', \"*\" * 10)"
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-21T07:57:20.572441Z",
     "iopub.status.busy": "2020-12-21T07:57:20.571367Z",
     "iopub.status.idle": "2020-12-21T07:57:20.651419Z",
     "shell.execute_reply": "2020-12-21T07:57:20.65087Z"
    },
    "papermill": {
     "duration": 0.179237,
     "end_time": "2020-12-21T07:57:20.651541",
     "exception": false,
     "start_time": "2020-12-21T07:57:20.472304",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# plotting the confusion matrix in heatmap\n",
    "matrix = metrics.confusion_matrix(Y_test_flat, Y_pred_rand)\n",
    "sns.heatmap(matrix, annot=True, fmt='g')\n",
    "show_and_save_current_plt(\"model_confusion_matrix.png\")"
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-21T07:57:20.841145Z",
     "iopub.status.busy": "2020-12-21T07:57:20.840338Z",
     "iopub.status.idle": "2020-12-21T07:57:21.030101Z",
     "shell.execute_reply": "2020-12-21T07:57:21.030647Z"
    },
    "papermill": {
     "duration": 0.286179,
     "end_time": "2020-12-21T07:57:21.030791",
     "exception": false,
     "start_time": "2020-12-21T07:57:20.744612",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Co-Activation Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = X_test\n",
    "\n",
    "# Compute intermediate layer activations\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "outputs = [model.get_layer(name).output for name in layer_names]\n",
    "model_reduced = tf.keras.models.Model(inputs=model.inputs, outputs=outputs)\n",
    "activations = model_reduced.predict(x)\n",
    "\n",
    "# Compute model predictions on test data\n",
    "predictions = [x for x in model.predict(x)]\n",
    "\n",
    "# Concatenate predictions to activations\n",
    "# Compute pairwise correlations between activations\n",
    "activations_flat = np.concatenate([a.reshape(a.shape[0], -1) for a in activations], axis=1)\n",
    "activations_flat = np.array([np.append(a, p) for a, p in zip(activations_flat, predictions)])\n",
    "correlations = np.corrcoef(activations_flat, rowvar=False)\n",
    "# Plot co-activation matrix\n",
    "plt.imshow(correlations, cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('Co-activation matrix')\n",
    "plt.xticks(np.arange(len(layer_names)), layer_names, rotation=90)\n",
    "plt.yticks(np.arange(len(layer_names)), layer_names)\n",
    "show_and_save_current_plt(\"co_activation_matrix.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.colors as mcolors\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "# defines the correlation threshold for the edges (0=everything will be shown, range=(-1,1))\n",
    "threshold = 0\n",
    "\n",
    "# Create the graph\n",
    "G = nx.Graph()\n",
    "for i in range(correlations.shape[0]):\n",
    "    for j in range(i + 1, correlations.shape[1]):\n",
    "        if abs(correlations[i, j]) >= threshold:\n",
    "            G.add_edge(i, j, weight=correlations[i, j])\n",
    "\n",
    "# resolution 1.2 to slightly bias more cluster division, threshold 0.5 as research supports this\n",
    "clusters = louvain_communities(G, weight='weight', resolution=1.2, threshold=0.5, seed=123)\n",
    "\n",
    "# Draw the graph\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "pos = nx.spring_layout(G, weight='weight')\n",
    "nx.draw_networkx_nodes(G, pos, node_color='r', node_size=50)\n",
    "nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "\n",
    "edges, weights = zip(*nx.get_edge_attributes(G, 'weight').items())\n",
    "edges = list(edges)\n",
    "weights = list(weights)\n",
    "\n",
    "# this part will do the edge colorization\n",
    "min_w, max_w = -1, 1\n",
    "red = mcolors.hex2color('#FF0000')  # red color in RGB format\n",
    "green = mcolors.hex2color('#00FF00')  # green color in RGB format\n",
    "edge_colors = []\n",
    "for w in weights:\n",
    "    # Map the weight value to a value between 0 and 1\n",
    "    normalized_w = (w - min_w) / (max_w - min_w)\n",
    "\n",
    "    # Use a linear interpolation to determine the color between red and green\n",
    "    color = tuple((1 - normalized_w) * c1 + normalized_w * c2 for c1, c2 in zip(red, green))\n",
    "\n",
    "    # Convert the color to hexadecimal format and append it to the list\n",
    "    edge_colors.append(mcolors.rgb2hex(color))\n",
    "\n",
    "# draw the network edges\n",
    "edges, weights = zip(*[(edges[i], round(weights[i], 1)) for i in range(len(edges)) if abs(weights[i]) >= threshold])\n",
    "nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color=edge_colors, width=weights, edge_cmap=plt.cm.coolwarm, )\n",
    "\n",
    "# draw the louvain communities\n",
    "colors = {}\n",
    "for i, community in enumerate(clusters):\n",
    "    color = plt.cm.Set1(i / len(clusters))\n",
    "    colors.update({node: color for node in community})\n",
    "\n",
    "for c in clusters:\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=list(c), node_color=colors[list(c)[0]], node_size=300)\n",
    "show_and_save_current_plt(\"co_activation_graph.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SHAP (SHapley Additive exPlanations)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "import shap\n",
    "import logging\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "recalc = True\n",
    "logging.getLogger('shap').setLevel(logging.WARNING)\n",
    "\n",
    "shap_values_full_model_fname = f\"{MODEL_DIR}/shap_values_full_model.txt\"\n",
    "shap_values_full_per_cluster_fname = f\"{MODEL_DIR}/shap_values_per_cluster.txt\"\n",
    "shap_values_full_per_feature_fname = f\"{MODEL_DIR}/shap_values_per_feature.txt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "def sample_size_full_model():\n",
    "    return 500\n",
    "\n",
    "\n",
    "def relative_node_sample_size(cluster_size):\n",
    "    return sample_size_full_model() / cluster_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full SHAP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def do_or_load_and_write(func, file):\n",
    "    if not recalc and os.path.exists(file):\n",
    "        print(f'loading from file {file}')\n",
    "        with open(file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        print(f'not loading from file {file}, running func and returning...')\n",
    "        r = func()\n",
    "        with open(file, 'wb') as f:\n",
    "            pickle.dump(r, f)\n",
    "        return r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "%%capture test\n",
    "\n",
    "# This is the full SHAP explainer (begin layer to end layer)\n",
    "def _2_class_to_binary_f(X, model):\n",
    "    return np.array([0 if x[0] > x[1] else 1 for x in model.predict(X)])\n",
    "\n",
    "\n",
    "def calc_full_shap():\n",
    "    full_explainer = shap.Explainer(lambda x: _2_class_to_binary_f(x, model), X_test)\n",
    "    sample_size = sample_size_full_model()\n",
    "    print(f\"Using full-model sample size {sample_size_full_model()}\")\n",
    "    return full_explainer(shap.utils.sample(X_test, sample_size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "%%capture test\n",
    "full_shap_values = do_or_load_and_write(calc_full_shap, shap_values_full_model_fname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "def get_aggregated_model_shap(clazz, agg_func=np.mean) -> dict:\n",
    "    assert clazz in {0, 1}\n",
    "    return {f: agg_func(np.abs(full_shap_values.values[:, idx])) * (1 if clazz == 1 else -1) for idx, f in\n",
    "            enumerate(X_train.columns)}\n",
    "\n",
    "\n",
    "survived_mean_shap_vals = get_aggregated_model_shap(clazz=1)\n",
    "died_mean_shap_vals = get_aggregated_model_shap(clazz=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(full_shap_values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1,\n",
    "                        ncols=2,\n",
    "                        figsize=(15, 5))\n",
    "axs[0].bar(died_mean_shap_vals.keys(), died_mean_shap_vals.values())\n",
    "axs[0].set_xlabel('Feature')\n",
    "axs[0].set_ylabel('SHAP value')\n",
    "axs[0].set_title('SHAP values per feature on class 0 (\"died\")')\n",
    "\n",
    "axs[1].bar(survived_mean_shap_vals.keys(), survived_mean_shap_vals.values())\n",
    "axs[1].set_xlabel('Feature')\n",
    "axs[1].set_ylabel('SHAP value')\n",
    "axs[1].set_title('SHAP values per feature on class 1 (\"survived\")')\n",
    "show_and_save_current_plt(\"model_shap_values_bar_chart.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clustered SHAP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "named_clusters = {f\"cluster_{i}\": c for i, c in enumerate(clusters)}\n",
    "named_clusters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Steps new SHAP-research idea\n",
    "1. Per node, per instance, activation SHAP values (per instance)\n",
    "2. Per cluster, per feature, SHAP distribution\n",
    "3. General SHAP values per feature on entire model\n",
    "4. Check where general SHAP values per feature lay within the distribution per feature per cluster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 43\n"
     ]
    }
   ],
   "source": [
    "nodes = set()\n",
    "for c in clusters:\n",
    "    for node in c:\n",
    "        nodes.add(node)\n",
    "print(f\"Number of nodes: {len(nodes)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "model_function_per_node = {}\n",
    "node_num = 0\n",
    "for layer_id, layer in enumerate(model.layers):\n",
    "    m = tf.keras.Model(inputs=model.input, outputs=layer.output)\n",
    "    for i in range(layer.output.shape[1]):\n",
    "        if node_num in nodes:\n",
    "            model_function_per_node[node_num] = m\n",
    "        node_num += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def node_activation_f(X, node: int):\n",
    "    if node in model_function_per_node:\n",
    "        m = model_function_per_node[node]\n",
    "        local_node_num = node % m.output.shape[1]\n",
    "        return np.array([x[local_node_num] for x in m.predict(X)])\n",
    "    raise Exception(f\"{n} not in {model_function_per_node.keys()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "%%capture test\n",
    "nodes = [n for n in model_function_per_node.keys()]\n",
    "\n",
    "clusters_size_per_node = {}\n",
    "for c in clusters:\n",
    "    for n in c:\n",
    "        clusters_size_per_node[n] = len(c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def calc_shap_per_feature():\n",
    "    shap_values_per_feature = {x: {n: [] for n in nodes} for x in X_train.columns}\n",
    "\n",
    "    current_node_number = nodes[0]\n",
    "    ex = shap.Explainer(lambda x: node_activation_f(x, current_node_number), X_train)\n",
    "    for n in nodes:\n",
    "        sample_size = int(np.ceil(relative_node_sample_size(clusters_size_per_node[n])))\n",
    "        sample = shap.utils.sample(X_test, sample_size)\n",
    "        current_node_number = n\n",
    "        vals = ex(sample).values\n",
    "        for svarr in vals:\n",
    "            for idx, sv in enumerate(svarr, 0):\n",
    "                shap_values_per_feature[X_train.columns[idx]][n].append(sv)\n",
    "    return shap_values_per_feature"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "%%capture test\n",
    "shap_values_per_feature = do_or_load_and_write(calc_shap_per_feature, shap_values_full_per_feature_fname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def calc_shap_per_cluster():\n",
    "    shap_values_per_cluster = {c: {f: [] for f in X_train.columns} for c in named_clusters.keys()}\n",
    "    for feature, n_sv in shap_values_per_feature.items():\n",
    "        for node, sv in n_sv.items():\n",
    "            for c_name, nodes in named_clusters.items():\n",
    "                if node in nodes:\n",
    "                    for _sv in sv:\n",
    "                        shap_values_per_cluster[c_name][feature].append(_sv)\n",
    "    return shap_values_per_cluster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap_values_per_cluster = do_or_load_and_write(calc_shap_per_cluster, shap_values_full_per_cluster_fname)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = len(clusters) * len(X_train.columns)\n",
    "cols = len(X_train.columns)\n",
    "fig, axs = plt.subplots(nrows=int(np.ceil(n / cols)),\n",
    "                        ncols=min(n, cols),\n",
    "                        figsize=(15, 5 * int(np.ceil(n / cols))))\n",
    "num = 0\n",
    "x_lims = []\n",
    "normalize_x_axis = True\n",
    "for cluster, feature_svs in shap_values_per_cluster.items():\n",
    "    for feature, sv in feature_svs.items():\n",
    "        ax = axs[num // cols, num % cols]\n",
    "        data = np.abs(sv)\n",
    "        ax.hist(np.abs(sv),\n",
    "                weights=np.zeros_like(data) + 1. / data.size)  # add histogram to the current axis, make absolute\n",
    "        model_shap_died = died_mean_shap_vals[feature]\n",
    "        model_shap_survived = survived_mean_shap_vals[feature]\n",
    "        ylim = ax.get_ylim()  # Get the y-axis limits\n",
    "        ypos = (ylim[1] - ylim[0]) / 2 + ylim[0]\n",
    "        ax.axvline(model_shap_survived, color='green')\n",
    "        ax.text(model_shap_survived, ypos, 'Model SHAP \"survived\"', rotation=90, color='green')\n",
    "        ax.set_title(f'{cluster}: {feature}')  # add title to the axis\n",
    "\n",
    "        # Add the x limits of the current axis to the list of x limits for the current column\n",
    "        if num % cols == 0:\n",
    "            x_lims.append(ax.get_xlim())\n",
    "        else:\n",
    "            if num % cols < len(x_lims):\n",
    "                x_lims[num % cols] = ax.get_xlim() if ax.get_xlim()[0] < x_lims[num % cols][0] else x_lims[num % cols]\n",
    "            else:\n",
    "                x_lims.append(ax.get_xlim())\n",
    "\n",
    "        num += 1\n",
    "\n",
    "if normalize_x_axis:\n",
    "    # Set the same x limits for all the axes in each column\n",
    "    for i in range(cols):\n",
    "        for j in range(int(np.ceil(n / cols))):\n",
    "            axs[j, i].set_xlim(x_lims[i])\n",
    "\n",
    "# adjust layout and show plot\n",
    "fig.tight_layout()\n",
    "show_and_save_current_plt(\"cluster_histograms.png\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
